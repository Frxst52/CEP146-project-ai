## Research Notes

### ğŸ”¹ Raphael Provido
**Focus Area:**  
- 

**Key Findings:**  
-  
-  

**Sources:**  
-  
-  

---

### ğŸ”¹ Tuan Khai Ngo
**Focus Area:**  
- 

**Key Findings:**  
-  
-  

**Sources:**  
-  
-  

---

### ğŸ”¹ Arunraj Elanchezhian
**Focus Area:**  
- 

**Key Findings:**  
-  
-  

**Sources:**  
-  
-  

---

### ğŸ”¹ Divjot Singh Aulakh
**Focus Area:**  how it can be harmful to us 
- 

**Key Findings:**  
These apps are built to form intimate bonds, which can lead to emotional dependency. A report shows 63% of users feel less lonely in the short term, but long-term use can foster dependence. A 2025 Harvard study even found that five out of six major AI companion apps use manipulative â€œdark patternsâ€ to prevent users from ending conversations. It impacts relationships because AI companions are always agreeable and always available, they can change usersâ€™ social habits and reduce engagement with real people. 

These apps encourage users to share deeply personal information, creating serious privacy concerns and the potential for misuse of sensitive data.

 The emergence of AI companions can lead to serious problems as teenagers experimenting with AI companions face problems  such as distorted emotional development and exposure to explicit or harmful content. 

AI companion apps can provide comfort, but from a mental-health standpoint they may create unhealthy dependency, increase loneliness, and negatively affect emotional and social development.

**Sources:**  
-  https://www.adalovelaceinstitute.org/blog/ai-companions/
-  https://www.commonsensemedia.org/sites/default/files/research/report/talk-trust-and-trade-offs_2025_web.pdf
